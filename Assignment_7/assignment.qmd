---
title: "Assignment 7"
subtitle: "DSAN Bootcamp 2024"
output: html
---

## K-Means Clustering: Overview

Let's create a function for the $K$-means clustering algorithm, in R!

![](c1.png)

**Details of the Above Figure:**

The progress of the $K$-means algorithm with $K = 3$.

* Top left: The observations are shown.
* Top center: In Step 1 of the algorithm, each observation is randomly assigned to a cluster using some kind of a label. It could be "1", "2" or "green", "red" etc.
* Top right: In Step 2(a), the cluster centroids are computed by taking the mean of each cluster assignment. These are shown as large colored disks. Initially the centroids are almost completely overlapping because the initial cluster assignments were chosen at random.
* Bottom left: In Step 2(b), each observation is assigned to the nearest centroid. This is done by computing the euclidean distance between each point and each of the cluster centers. Each point is assigned to the centroid that resulted in the smallest euclidean distance. 
* Bottom center: Step 2(a) is once again performed, leading to new cluster centroids.
* Bottom right: The results obtained after 10 iterations.

Think about:

* The iterative steps for the algorithm. What is the starting point? What is the update rule for the centroids? What is the update rule for the "assignment" of each data point to a cluster?
* What data structures might you use for implementing this algorithm? DataFrames or vectors would be fine, just think about what you need to keep track of during each iteration.
* Within your implementation, create a way to visualize our 2D dataset during each implementation. That is, it is helpful to visualize the cluster "assignments" and centroids during each iteration. 

Let's first look at a simple simulated example in which there are only two real clusters in the data. We will create a dataset where the first $N/2$ observations have a mean shift relative to the next $N/2$ observations.

```{r}
#| label: cluster-generation
#| warning: false
library(tidyverse)
set.seed(2024)
N <- 200
gen_clusters = function(N, dist) {
  cluster_size <- round(N / 2)
  mu_1 <- c(-dist/2,-dist/2)
  mu_2 <- c(dist/2, dist/2)
  Sigma <- matrix(c(1, 0, 0, 1), nrow=2, ncol=2, byrow=TRUE)
  c1_points <- MASS::mvrnorm(cluster_size, mu_1, Sigma)
  colnames(c1_points) <- c("X1", "X2")
  c1_points <- as_tibble(c1_points)
  c1_points <- c1_points |> mutate(label = "1")
  c2_points <- MASS::mvrnorm(cluster_size, mu_2, Sigma)
  colnames(c2_points) <- c("X1", "X2")
  c2_points <- as_tibble(c2_points)
  c2_points <- c2_points |> mutate(label = "2")
  return(rbind(c1_points, c2_points))
}
linear_data <- gen_clusters(N, 4.5)
linear_data |>
  ggplot(aes(x=X1, y=X2, color=label)) +
  geom_point() +
  theme_classic()
```

## Writing the K-Means Clustering Function

### Question 1

Make a copy of `linear_data` named `df`, and then display the first six rows of `df` by using the pipe operator `|>` and R's built-in `head()` function.

```{r}
#| label: q1-response
# Your code here

df <- linear_data

df |> head(6)
```

### Question 2

Create a vector named `"labels"` containing the value `"1"` 100 times, followed by the value `"2"` 100 times.

```{r}
#| label: q2-response
# Your code here

labels <- c(rep("1", 100), rep("2", 100))

labels
```

### Question 3

Create a new vector named `cl_labels`, by taking a random sample of size 200 from `labels` without replacement.

```{r}
#| label: q3-response
# Your code here

cl_labels <- sample(labels, size = 200, replace = FALSE)

cl_labels
```

### Question 4

Create a new column in `df` named `label`, with the entries in `cl_labels` as its values.

```{r}
#| label: q4-response
# Your code here

library(dplyr)
df <- df |> mutate(label = cl_labels)

head(df)

```

### Question 5

Calculate the cluster centroids, by creating a variable named `c1` representing the centroid (mean vector) of all points with the label `"1"` and `c2` representing the centroid (mean vector) of all points with the label `"2"`.

```{r}
#| label: q5-response
# Your code here

c1 <- df |> filter(label == "1") |> summarize(X1 = mean(X1), X2 = mean(X2))
c2 <- df |> filter(label == "2") |> summarize(X1 = mean(X1), X2 = mean(X2))

c1
c2
```

### Question 6

Plot the entries in `df`, with `X1` on the x-axis and `X2` on the y-axis, and color them according to the `label` column. Are they separated yet?

```{r}
#| label: q6-response
# Your code here

library(ggplot2)

ggplot(df, aes(x = X1, y = X2, color = label)) +
  geom_point() +
  theme_classic() +
  labs(title = "Scatter Plot of df with X1 and X2 Colored by Label")
```

### Question 7

Write a function called `compute_distance` which takes in coordinates `X1` and `X2` for a given point, along with coordinates `X1c` and `X2c` for a given cluster centroid, and returns the euclidean distance between these two points. Then, test that your function works correctly by calling it to compute the distance between the example point $X = (0.5, 1.0)$ and the example centroid $c = (3.2,3.0)$ (the answer, for this case, should be `3.36006`)

```{r}
#| label: q7-response
# Your code here

compute_distance <- function(X1, X2, X1c, X2c) {
  distance <- sqrt((X1 - X1c)^2 + (X2 - X2c)^2)
  return(distance)
}

example_distance <- compute_distance(0.5, 1.0, 3.2, 3.0)
example_distance
```

### Question 8

Next you need to re-assign the cluster labels according to the minimum distance between the cluster centroid and each point. You can accomplish this using **for loops** over the points (rows) in `df`.

In Questions 8.1 and 8.2 you will write functions for carrying out the two main steps of the $K$-means clustering algorithm, and then in Question 8.3 you will run the two functions 10 times. Then, in Question 9, you will visualize the resulting clusters, to see whether the 10 iterations worked!

#### Question 8.1

Write a function called `update_labels`. This function should take in the following arguments:

* `input_df`: A `data.frame` where each row corresponds to a point in the dataset
* `input_c1`: A centroid for the label `"1"` (represented as a length-2 `numeric` vector)
* `input_c2`: A centroid for the label `"2"` (represented as a length-2 `numeric` vector)

The function should then loop over each point (row) in `input_df`, using an an if-else statement to check which cluster centroid (`input_c1` or `input_c2`) is closer to the point and re-assign the point's label accordingly. Once all of the points (rows in `input_df`) have been processed, the function should return the newly-updated `input_df`.

```{r}
#| label: q8.1-response
# Your code here

compute_distance <- function(X1, X2, X1c, X2c) {
  distance <- sqrt((X1 - X1c)^2 + (X2 - X2c)^2)
  return(distance)
}

update_labels <- function(input_df, input_c1, input_c2) {
  for (i in 1:nrow(input_df)) {
    X1 <- input_df$X1[i]
    X2 <- input_df$X2[i]
    
    dist_to_c1 <- compute_distance(X1, X2, input_c1[1], input_c1[2])
    dist_to_c2 <- compute_distance(X1, X2, input_c2[1], input_c2[2])
  
    if (dist_to_c1 < dist_to_c2) {
      input_df$label[i] <- "1"
    } else {
      input_df$label[i] <- "2"
    }
  }
  
  return(input_df)
}

initial_c1 <- c(mean(df$X1[df$label == "1"]), mean(df$X2[df$label == "1"]))
initial_c2 <- c(mean(df$X1[df$label == "2"]), mean(df$X2[df$label == "2"]))

updated_df <- update_labels(df, initial_c1, initial_c2)
head(updated_df)
```

#### Question 8.2

Write another function called `update_centroids`. This function should accept just one argument, called `input_df`, and should calculate new cluster centroids for each label (`"1"` and `"2"`), similar to what you did in Question 5. Once the centroids have been calculated, they should be returned as a `list`, whose first element is the centroid (the length-2 `numeric` vector) of the points with label `"1"` and whose sectond element is the centroid (the length-2 `numeric` vector) of the points with label `"2"`.

```{r}
#| label: q8.2-response
# Your code here

update_centroids <- function(input_df) {
  c1 <- input_df |> 
    filter(label == "1") |> 
    summarize(X1 = mean(X1), X2 = mean(X2)) |> 
    as.numeric()
  
  c2 <- input_df |> 
    filter(label == "2") |> 
    summarize(X1 = mean(X1), X2 = mean(X2)) |> 
    as.numeric()
  
  return(list(c1, c2))
}

centroids <- update_centroids(updated_df)
centroids
```

#### Question 8.3

Now you will use the functions written in Questions 8.1 and 8.2 to carry out the $K$-means algorithm. Using a for loop, utilize these two functions to update `df`, `c1`, and `c2` **ten times** in total.

```{r}
#| label: q8.3-response
# Your code here

c1 <- c(mean(df$X1[df$label == "1"]), mean(df$X2[df$label == "1"]))
c2 <- c(mean(df$X1[df$label == "2"]), mean(df$X2[df$label == "2"]))

for (i in 1:10) {
  df <- update_labels(df, c1, c2)
  
  centroids <- update_centroids(df)
  c1 <- centroids[[1]]
  c2 <- centroids[[2]]
}

head(df)

c1
c2
```

### Question 9

Plot the points in `df`, the same way you did in Question 6, to see if two visibly-separated clusters have emerged after the ten iterations of the $K$-means clustering algorithm you carried out in the previous question.

```{r}
#| label: q9-response
# Your code here

ggplot(df, aes(x = X1, y = X2, color = label)) +
  geom_point() +
  theme_classic() +
  labs(title = "Scatter Plot of df after 10 Iterations of K-Means Clustering",
       x = "X1",
       y = "X2")
```
